{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "29d46753",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#Importing same data used in Homework 1\n",
    "from sklearn.datasets import fetch_openml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ef13bf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the mammography dataset\n",
    "data = fetch_openml('mammography', as_frame=True)\n",
    "X, y = data.data, data.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "42155303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting X to a NumPy array\n",
    "X = X.to_numpy()\n",
    "\n",
    "# Converting y to a NumPy array and ensure it contains numeric values\n",
    "y = y.astype(float).to_numpy().reshape(-1, 1)\n",
    "\n",
    "# Standardizing the features (mean=0, std=1)\n",
    "X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e3a81aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a column of ones for the intercept\n",
    "X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "\n",
    "# Defining the linear regression function and calculate the loss\n",
    "def linear_regression(beta, X, y):\n",
    "    y_pred = X.dot(beta)\n",
    "    loss = np.sum((y_pred - y) ** 2)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "322210d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the gradient descent function\n",
    "def gradient_descent(X, y, learning_rate, num_iterations):\n",
    "    n = X.shape[0]\n",
    "    p = X.shape[1]\n",
    "    \n",
    "    # Initializing the beta and best_beta to 0; Initialize best_loss.\n",
    "    beta = np.zeros((p, 1))\n",
    "    best_loss = float('inf')\n",
    "    best_beta = np.zeros((p, 1))\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        # Computing the gradient of the loss function at beta\n",
    "        y_pred = X.dot(beta)\n",
    "        gradient = 2 * X.T.dot(y_pred - y) / n\n",
    "        \n",
    "        # Updating beta\n",
    "        beta -= learning_rate * gradient\n",
    "        \n",
    "        # Keeping track of the best seen so far loss and parameters\n",
    "        current_loss = linear_regression(beta, X, y)\n",
    "        if current_loss < best_loss:\n",
    "            best_loss = current_loss\n",
    "            best_beta = beta\n",
    "        \n",
    "        # Printing beta and loss update within the for loop\n",
    "        if i < 10 or i > 29990:\n",
    "            print(f\"Iteration: {i}, Beta Values: {beta.flatten()}\")\n",
    "            print(f\"Best Loss: {current_loss}\\n\")\n",
    "    \n",
    "    # Returning a dictionary of final results\n",
    "    return {'beta': best_beta, 'loss': best_loss}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f5f465fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the learning rate and number of iterations\n",
    "learning_rate = 0.0001\n",
    "num_iterations = 30000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95c5a7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Beta Values: [-1.90700170e-04  7.20606774e-06 -1.09054622e-06 -2.84963172e-06\n",
      "  1.55252558e-05  2.85046864e-05  1.01845843e-05]\n",
      "Best Loss: 11178.797292819947\n",
      "\n",
      "Iteration: 1, Beta Values: [-3.81362200e-04  1.44087788e-05 -2.18266811e-06 -5.70000168e-06\n",
      "  3.10430768e-05  5.70014193e-05  2.03620397e-05]\n",
      "Best Loss: 11174.59630933835\n",
      "\n",
      "Iteration: 2, Beta Values: [-5.71986097e-04  2.16081349e-05 -3.27636446e-06 -8.55110899e-06\n",
      "  4.65534668e-05  8.54902017e-05  3.05323699e-05]\n",
      "Best Loss: 11170.397048822611\n",
      "\n",
      "Iteration: 3, Beta Values: [-7.62571870e-04  2.88041377e-05 -4.37163407e-06 -1.14029527e-05\n",
      "  6.20564295e-05  1.13971036e-04  4.06955783e-05]\n",
      "Best Loss: 11166.199510540471\n",
      "\n",
      "Iteration: 4, Beta Values: [-9.53119525e-04  3.59967889e-05 -5.46847571e-06 -1.42555321e-05\n",
      "  7.75519684e-05  1.42443926e-04  5.08516685e-05]\n",
      "Best Loss: 11162.003693760002\n",
      "\n",
      "Iteration: 5, Beta Values: [-1.14362907e-03  4.31860904e-05 -6.56688817e-06 -1.71088460e-05\n",
      "  9.30400874e-05  1.70908874e-04  6.10006442e-05]\n",
      "Best Loss: 11157.809597749609\n",
      "\n",
      "Iteration: 6, Beta Values: [-1.33410052e-03  5.03720437e-05 -7.66687025e-06 -1.99628937e-05\n",
      "  1.08520790e-04  1.99365882e-04  7.11425088e-05]\n",
      "Best Loss: 11153.617221778037\n",
      "\n",
      "Iteration: 7, Beta Values: [-1.52453387e-03  5.75546506e-05 -8.76842072e-06 -2.28176743e-05\n",
      "  1.23994080e-04  2.27814955e-04  8.12772660e-05]\n",
      "Best Loss: 11149.42656511436\n",
      "\n",
      "Iteration: 8, Beta Values: [-1.71492913e-03  6.47339128e-05 -9.87153839e-06 -2.56731869e-05\n",
      "  1.39459961e-04  2.56256094e-04  9.14049194e-05]\n",
      "Best Loss: 11145.237627027993\n",
      "\n",
      "Iteration: 9, Beta Values: [-1.90528631e-03  7.19098319e-05 -1.09762220e-05 -2.85294305e-05\n",
      "  1.54918437e-04  2.84689302e-04  1.01525472e-04]\n",
      "Best Loss: 11141.050406788676\n",
      "\n",
      "Iteration: 29991, Beta Values: [-0.95113499  0.02121226 -0.02517487 -0.01669687  0.06144328  0.13308737\n",
      " -0.04566967]\n",
      "Best Loss: 755.1169189606762\n",
      "\n",
      "Iteration: 29992, Beta Values: [-0.95113547  0.02121223 -0.02517499 -0.01669672  0.06144441  0.13308757\n",
      " -0.04567095]\n",
      "Best Loss: 755.1165615962914\n",
      "\n",
      "Iteration: 29993, Beta Values: [-0.95113594  0.02121219 -0.02517511 -0.01669657  0.06144553  0.13308776\n",
      " -0.04567222]\n",
      "Best Loss: 755.1162042595056\n",
      "\n",
      "Iteration: 29994, Beta Values: [-0.95113641  0.02121216 -0.02517523 -0.01669642  0.06144666  0.13308796\n",
      " -0.04567349]\n",
      "Best Loss: 755.1158469503141\n",
      "\n",
      "Iteration: 29995, Beta Values: [-0.95113688  0.02121212 -0.02517535 -0.01669628  0.06144779  0.13308815\n",
      " -0.04567477]\n",
      "Best Loss: 755.1154896687112\n",
      "\n",
      "Iteration: 29996, Beta Values: [-0.95113736  0.02121209 -0.02517548 -0.01669613  0.06144892  0.13308835\n",
      " -0.04567604]\n",
      "Best Loss: 755.115132414692\n",
      "\n",
      "Iteration: 29997, Beta Values: [-0.95113783  0.02121205 -0.0251756  -0.01669598  0.06145005  0.13308854\n",
      " -0.04567732]\n",
      "Best Loss: 755.1147751882511\n",
      "\n",
      "Iteration: 29998, Beta Values: [-0.9511383   0.02121202 -0.02517572 -0.01669583  0.06145117  0.13308874\n",
      " -0.04567859]\n",
      "Best Loss: 755.1144179893834\n",
      "\n",
      "Iteration: 29999, Beta Values: [-0.95113878  0.02121199 -0.02517584 -0.01669569  0.0614523   0.13308893\n",
      " -0.04567986]\n",
      "Best Loss: 755.1140608180835\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Running the gradient descent function\n",
    "result = gradient_descent(X, y, learning_rate, num_iterations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2ec818bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Theta (Coefficients): [-0.95113878  0.02121199 -0.02517584 -0.01669569  0.0614523   0.13308893\n",
      " -0.04567986]\n",
      "Best Loss: 755.1140608180835\n"
     ]
    }
   ],
   "source": [
    "# Printing the optimal theta (coefficients)\n",
    "print(\"Optimal Theta (Coefficients):\", result['beta'].flatten())\n",
    "print(\"Best Loss:\", result['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e17d83be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To compare the Ordinary Least Squares (OLS) solutions from Homework 1 to the gradient descent estimate\n",
    "#we can calculate the difference between the coefficients obtained by these two methods\n",
    "\n",
    "# Loading the coefficients from Homework 1 (using 4 features)\n",
    "ols_coefficients = np.array([1.172653, -0.235387, -0.036558, 0.137994])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "face2412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the coefficients obtained from gradient descent\n",
    "gradient_descent_coefficients = result['beta'].flatten()[:4]  # Use the first 4 coefficients\n",
    "\n",
    "# Calculate the absolute differences between OLS and gradient descent coefficients\n",
    "differences = np.abs(ols_coefficients - gradient_descent_coefficients)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8b37c28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient Differences:\n",
      "Beta_0: 2.123791775056586\n",
      "Beta_1: 0.25659898559630245\n",
      "Beta_2: 0.011382159761906742\n",
      "Beta_3: 0.15468968616197254\n"
     ]
    }
   ],
   "source": [
    "# Print the coefficient differences\n",
    "print(\"Coefficient Differences:\")\n",
    "for i in range(len(ols_coefficients)):\n",
    "    print(f\"Beta_{i}: {differences[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e7c99e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
